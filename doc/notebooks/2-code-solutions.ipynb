{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Code Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where we're at\n",
    "Up to this point we've coded up some basic infrastructure that makes it easy to run ARC prize items, where out business logic including Reflexion and giving the agent plenty of time to think fits into about a page.\n",
    "\n",
    "We've run the first 20 problems in the training set and got 3/20 = 15%.  This is showing some non-zero signs of life and is a good baseline to start improving.\n",
    "\n",
    "## Where we go from here\n",
    "Looking at the sessions where it failed, it looks like it understands what to do but is having trouble with the details of the execution.  For example, on one of the items involving a flood fill, the agent understands what it needs to do and mostly does it, but has trouble geting every single cell right.\n",
    "\n",
    "## Hypothesis: Python DSL\n",
    "Can we do a thing where it writes code to do it?  Others on the leaderboard have done this using their own DSL and fine tuning.  Maybe we can make a collection of Python functions with libraries that make a lot of these tasks as easy as practical.  This way we leverage the model's deep understanding of Python.\n",
    "\n",
    "## Next steps:\n",
    "* [ ] **Verify**: Does it actually understand the patterns? Review the cases where it failed.  If it can code strong, will that actually help? It won't help if the agent did not actually understand the pattern.  My hypothesis is grounded in the assumption that mostly it does understand what it needs to do, but it is struggling with the **execution** of what it needs to do.  Verify this assumption.\n",
    "* [ ] **Prompt**: Put together a prompt including what you propose doing, and see how the LLM does with that.  This way I can verify the promise of this approach before I actually build all of it. This is a common trick I use with LLM projects: verify the promise of the approach before I build the infrastructure needed to make it happen.\n",
    "* [ ] **Infrastructure:**  Assuming the above two things turn out like I expect, some ideas of what to do with it are:\n",
    "    * **Disjoint sets**.  Make a great abstraction for working with disjoint sets, as I have done in the past for working with datacenter tiles.\n",
    "    * **Coding agent.**  Make a coding agent\n",
    "    * **Module describer.** Make a thing that describes all the public classes and methods of a module, so that I'm spending more time writing code and less time writng prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
