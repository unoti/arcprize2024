{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Code Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where we're at\n",
    "Up to this point we've coded up some basic infrastructure that makes it easy to run ARC prize items, where out business logic including Reflexion and giving the agent plenty of time to think fits into about a page.\n",
    "\n",
    "We've run the first 20 problems in the training set and got 3/20 = 15%.  This is showing some non-zero signs of life and is a good baseline to start improving.\n",
    "\n",
    "## Where we go from here\n",
    "Looking at the sessions where it failed, it looks like it understands what to do but is having trouble with the details of the execution.  For example, on one of the items involving a flood fill, the agent understands what it needs to do and mostly does it, but has trouble geting every single cell right.\n",
    "\n",
    "## Hypothesis: Python DSL\n",
    "Can we do a thing where it writes code to do it?  Others on the leaderboard have done this using their own DSL and fine tuning.  Maybe we can make a collection of Python functions with libraries that make a lot of these tasks as easy as practical.  This way we leverage the model's deep understanding of Python.\n",
    "\n",
    "## Next steps:\n",
    "* [ ] **Verify agent understanding**: Does it actually understand the patterns? Review the cases where it failed.  If it can code strong, will that actually help? It won't help if the agent did not actually understand the pattern.  My hypothesis is grounded in the assumption that mostly it does understand what it needs to do, but it is struggling with the **execution** of what it needs to do.  Verify this assumption.\n",
    "* [ ] **Prompt**: Put together a prompt including what you propose doing, and see how the LLM does with that.  This way I can verify the promise of this approach before I actually build all of it. This is a common trick I use with LLM projects: verify the promise of the approach before I build the infrastructure needed to make it happen.\n",
    "* [ ] **Evaluation Infrastructure:** I should set up an infrastructure where I can rapidly try different approaches, leaving the old approaches intact maybe, and rapidly get feedback about how that worked out.  For example, retry the same set of 20 problems.  At the moment scores are stored in each session, but I should also create a summary score sheet stored as both json as well as markdown, showing the summary of what happened in the run.\n",
    "    * **Score sheet** for a run so we can quickly see which ones worked and didn't.  We want this to come out as a pandas dataframe so that I can rapidly filter and sort and such.  I may actually add human annotations, e.g. \"flood fill\" so I know what the strategy on the case is at a glance.  Because it's a pandas dataframe I'll probably save these as a csv.  Make a function that returns a markdown representation of the results.  Join in the descriptions of what the case is.  Make another function that makes all that for the last run that we did, so I can easily plop that into a notebook.  Could include [links to the cases like this](https://arcprize.org/play?task=06df4c85).\n",
    "    * **Images to improve human understanding**.  If I struggle to understand a given case by staring at the matrix then it may be worth the time investment to make img files that show the input and output cases to help me iterate on this faster.  I say *human* understanding because it's not clear to me that image recognition for the multi-modal models actually improves performance much.  Evidence says no, but I'm not totally sure.  If our agent understands the patterns of most of our failed use cases then that will be evidence that the image recognition doesn't matter too much to the agent.  I have observed that the LLM has no trouble at all understanding a big jumbled mess of unformatted JSON in a way that's superhuman, so it sure wouldn't surprise me if the image recognition doesn't help much.  So this is why I say the images would be to aid me in *human* understanding, rapidly remembering what the case is about when looking at a case.\n",
    "* [ ] **Coding Infrastructure:**  Assuming the above two things turn out like I expect, some ideas of what to do with it are:\n",
    "    * **Disjoint sets**.  Make a great abstraction for working with disjoint sets, as I have done in the past for working with datacenter tiles.\n",
    "    * **Coding agent.**  Make a coding agent\n",
    "    * **Module describer.** Make a thing that describes all the public classes and methods of a module, so that I'm spending more time writing code and less time writng prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Agent Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll review the cases and see if it looks like the LLM understood what the case was about. (I should be able to just push a button and get the dataframe or markdown of the last run; maybe I'll implement that later.)\n",
    "\n",
    "| Seq | Case | Result | What is it | Did agent get it |\n",
    "|-----|------|--------|------------|-------------------|\n",
    "| 1 | 007bbfb7 |fail | 3x3 expand to 9x9, replicate when set |  |\n",
    "|2 | 00d62c1b |fail | flood fill | |\n",
    "|3 | 017c7c7b| PASS | continue pattern to height 9 | |\n",
    "|4 | 025d127b| PASS | square up the bottom | |\n",
    "|5 | 045e512c | fail | directional color replicator | |\n",
    "|6 | 0520fde7 | fail | boolean AND on two 3x3s | |\n",
    "|7 | 05269061 | fail | diagonal pattern expander | |\n",
    "|8 | 05f2a901 | fail | square sucks in the shape | |\n",
    "|9 | 06df4c85 | fail | connect same colors in the grid | |\n",
    "|10 | 08ed6ac7 | fail | size 1-4 histogram classifier | |\n",
    "|11 | 09629e4f | fail | find, copy and expand the section without blue | |\n",
    "|12 | 0962bcdd | fail | grow crystals | |\n",
    "|13 | 0a938d79 | fail | expand into stripes and continue | |\n",
    "|14 | 0b148d64 | fail | find the unique color cluster and output it | |\n",
    "|15 | 0ca9ddb6 | fail | grow crystals 2, don't grow blue | |\n",
    "|16 | 0d3d703e | fail | simple color transform | |\n",
    "|17 | 0dfd9992 | PASS | Ginormous pattern fill | |\n",
    "|18 | 0e206a2e | fail |\n",
    "|19 | 10fcaaa3 | fail |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we just take a moment to bask in the glory that it actually solved #17 above? That's this one:\n",
    "![ginormous pattern fill](img/big-pattern-continue.png)\n",
    "\n",
    "So this thing may not be storming up the leaderboards, but it definitely has some intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
