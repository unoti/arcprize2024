{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backlog\n",
    "\n",
    "In this notbook we'll keep ideas of ways to extend this later. I'm collecting them here because as I go I keep getting ideas of ways to expand things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Session filenames should be based on the case id.  This will make it easier to find stuff.  Do this with a callable that makes the session name based on `app_context`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [ ] Go thru notebook 2 and transfer items from there into this backlog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We get 3 attempts. We need to take advantage of those!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fine tuning: do as many of these as you can stomach, providing thought traces.  Then fine tune. ??.  Profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The key problem is the LLM not being clever enough to understand the patterns/algorithms.  I did experiments where I basically handed it the solution and it wasn't bright enough to pick up on the possibility that I could be right.  \n",
    "* One possible path to success I can think of is to solve hundreds of problems carefully narrating my thought process, and fine tune.  Also unsure whether LORA fine tuning has enough parameter space to encapsulate those ways of thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
